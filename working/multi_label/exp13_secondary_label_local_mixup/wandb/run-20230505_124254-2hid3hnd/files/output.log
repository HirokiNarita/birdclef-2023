load pre-trained model :  /kaggle/working/pre-training/exp1_pre_attblock_fold-0_20230501_pretrained.pth
_IncompatibleKeys(missing_keys=['bn0.weight', 'bn0.bias', 'bn0.running_mean', 'bn0.running_var', 'bn0.num_batches_tracked', 'model.classifier.weight', 'model.classifier.bias', 'att_block.att.weight', 'att_block.att.bias', 'att_block.cla.weight', 'att_block.cla.bias'], unexpected_keys=['to_melspec_fn.spectrogram.window', 'to_melspec_fn.mel_scale.fb'])
#########################
#### Training
#### Fold: 1 | Image Size: (224, 313) | Model: tf_efficientnet_b1_ns | Batch Size: 128 | Scheduler: cos
#### Num Train: 19,627 | Num Valid: 3,381
  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:04<00:00,  4.45s/it]

  4%|▎         | 1/27 [00:04<01:47,  4.15s/it]
torch.Size([128, 1, 128, 469])

 15%|█▍        | 4/27 [00:06<00:26,  1.16s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 26%|██▌       | 7/27 [00:08<00:16,  1.18it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 37%|███▋      | 10/27 [00:10<00:13,  1.27it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 44%|████▍     | 12/27 [00:12<00:10,  1.38it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 59%|█████▉    | 16/27 [00:14<00:07,  1.50it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 70%|███████   | 19/27 [00:16<00:05,  1.53it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])


 89%|████████▉ | 24/27 [00:20<00:02,  1.27it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

100%|██████████| 27/27 [00:22<00:00,  1.22it/s]
Epoch: 1 | Train Loss: 0.9668199419975281 | Val Loss: 0.9581042969668353 | Val Padded_cmAP : 0.4546675116383344
  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:03<00:00,  3.22s/it]
  0%|          | 0/27 [00:00<?, ?it/s]

 11%|█         | 3/27 [00:08<00:49,  2.05s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 22%|██▏       | 6/27 [00:10<00:22,  1.09s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
 26%|██▌       | 7/27 [00:11<00:19,  1.03it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc1a7eeb560>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1466, in __del__
    self._shutdown_workers()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1430, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/conda/lib/python3.7/multiprocessing/process.py", line 140, in join
    res = self._popen.wait(timeout)
  File "/opt/conda/lib/python3.7/multiprocessing/popen_fork.py", line 45, in wait
    if not wait([self.sentinel], timeout):
  File "/opt/conda/lib/python3.7/multiprocessing/connection.py", line 921, in wait
    ready = selector.select(timeout)
  File "/opt/conda/lib/python3.7/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
 26%|██▌       | 7/27 [00:11<00:32,  1.63s/it]
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 138, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 405, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 127, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 395, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
