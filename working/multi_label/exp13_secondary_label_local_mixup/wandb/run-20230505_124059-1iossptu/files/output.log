load pre-trained model :  /kaggle/working/pre-training/exp1_pre_attblock_fold-0_20230501_pretrained.pth
_IncompatibleKeys(missing_keys=['bn0.weight', 'bn0.bias', 'bn0.running_mean', 'bn0.running_var', 'bn0.num_batches_tracked', 'model.classifier.weight', 'model.classifier.bias', 'att_block.att.weight', 'att_block.att.bias', 'att_block.cla.weight', 'att_block.cla.bias'], unexpected_keys=['to_melspec_fn.spectrogram.window', 'to_melspec_fn.mel_scale.fb'])
#########################
#### Training
#### Fold: 1 | Image Size: (224, 313) | Model: tf_efficientnet_b1_ns | Batch Size: 128 | Scheduler: cos
#### Num Train: 19,627 | Num Valid: 3,381
  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:07<00:00,  7.50s/it]
  0%|          | 0/27 [00:00<?, ?it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

  7%|▋         | 2/27 [00:07<01:18,  3.13s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 19%|█▊        | 5/27 [00:10<00:32,  1.49s/it]
torch.Size([128, 1, 128, 469])

 30%|██▉       | 8/27 [00:11<00:16,  1.15it/s]
torch.Size([128, 1, 128, 469])

 33%|███▎      | 9/27 [00:13<00:17,  1.01it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 44%|████▍     | 12/27 [00:15<00:12,  1.24it/s]
torch.Size([128, 1, 128, 469])

 52%|█████▏    | 14/27 [00:17<00:12,  1.03it/s]
torch.Size([128, 1, 128, 469])

 59%|█████▉    | 16/27 [00:18<00:08,  1.28it/s]
torch.Size([128, 1, 128, 469])

 67%|██████▋   | 18/27 [00:21<00:09,  1.05s/it]
torch.Size([128, 1, 128, 469])

 78%|███████▊  | 21/27 [00:23<00:05,  1.08it/s]
torch.Size([128, 1, 128, 469])

 81%|████████▏ | 22/27 [00:25<00:05,  1.13s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

 96%|█████████▋| 26/27 [00:27<00:00,  1.38it/s]
torch.Size([53, 1, 128, 469])
100%|██████████| 27/27 [00:28<00:00,  1.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:05<00:00,  5.66s/it]
  0%|          | 0/27 [00:00<?, ?it/s]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])

  7%|▋         | 2/27 [00:06<01:10,  2.82s/it]

 15%|█▍        | 4/27 [00:08<00:35,  1.52s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])


 30%|██▉       | 8/27 [00:11<00:16,  1.19it/s]

 37%|███▋      | 10/27 [00:14<00:23,  1.40s/it]
torch.Size([128, 1, 128, 469])
torch.Size([128, 1, 128, 469])
Process wandb_internal:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py", line 160, in wandb_internal
    thread.join()
  File "/opt/conda/lib/python3.7/threading.py", line 1044, in join
    self._wait_for_tstate_lock()
  File "/opt/conda/lib/python3.7/threading.py", line 1060, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 138, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 405, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 127, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 395, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
